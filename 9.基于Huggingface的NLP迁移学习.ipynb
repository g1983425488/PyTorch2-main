{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7f372e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 3844, 6407, 6427, 1368,  102,    0,    0]]) torch.Size([1, 8]) torch.int64\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0]]) torch.Size([1, 8]) torch.int64\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 0, 0]]) torch.Size([1, 8]) torch.int64\n",
      "[CLS] 测 试 语 句 [SEP] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载字典和分词工具,这里的checkpoint一般和预训练模型保持一致,两者是成对使用的\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "#试编码一句话,所有一句编码后都是等长的\n",
    "out = tokenizer(text='测试语句',\n",
    "                padding='max_length',\n",
    "                max_length=8,\n",
    "                return_tensors='pt',\n",
    "                truncation=True)\n",
    "\n",
    "#查看编码结果\n",
    "for k, v in out.items():\n",
    "    print(k, v, v.shape, v.dtype)\n",
    "\n",
    "#反编码\n",
    "print(tokenizer.decode(out['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1c4708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     validation: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 1200\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 1200\n",
       "     })\n",
       "     train: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 9600\n",
       "     })\n",
       " }),\n",
       " {'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "  'label': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "#在线加载数据集,国内网络环境得看运气\n",
    "#更多数据集:https://huggingface.co/datasets\n",
    "#dataset = load_dataset(path='lansinuote/ChnSentiCorp')\n",
    "\n",
    "#保存到磁盘\n",
    "#dataset.save_to_disk('data/ChnSentiCorp')\n",
    "\n",
    "#加载离线数据集\n",
    "dataset = load_from_disk('data/ChnSentiCorp')\n",
    "\n",
    "dataset, dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09d0638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221,\n",
       "         3175,  912, 8024, 3300, 4510, 1220, 2820, 3461, 4684, 2970, 1168, 6809,\n",
       "         3862, 6804, 8024, 1453, 1741, 7623, 7667,  510, 7608, 2443,  510, 1555,\n",
       "         1767,  510, 6631, 2356,  510, 3033,  855,  671, 2418,  936, 1059,  511,\n",
       "         6983, 2421, 6163,  934,  671, 5663, 8024,  852, 6820, 5050, 3146, 3815,\n",
       "          511, 3807, 3737, 1762, 1920, 1828, 4638, 2238, 7553, 8024, 1728, 3634,\n",
       "         2523, 2207, 8024,  679, 6814, 1957, 1036,  948, 3221, 1599, 3614,  511,\n",
       "         1259, 4638, 3193, 7623, 3221, 6205, 2466, 4638, 8024, 6820, 5050,  705,\n",
       "         2168,  511, 3302, 1218, 1408, 8024,  671, 5663,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#添加取数据时的回调函数,可以在这里完成数据的处理工作\n",
    "def f(data):\n",
    "    #编码句子\n",
    "    result = tokenizer(text=data['text'],\n",
    "                       padding='max_length',\n",
    "                       max_length=200,\n",
    "                       return_tensors='pt',\n",
    "                       truncation=True)\n",
    "\n",
    "    #label字段不能落下,添加到返回结果里\n",
    "    result['label'] = data['label']\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "dataset = dataset.with_transform(f, output_all_columns=False)\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb08812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 200]) torch.int64\n",
      "token_type_ids torch.Size([8, 200]) torch.int64\n",
      "attention_mask torch.Size([8, 200]) torch.int64\n",
      "label torch.Size([8]) torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset['train'],\n",
    "                                     batch_size=8,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for k, v in next(iter(loader)).items():\n",
    "    print(k, v.shape, v.dtype)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "460f291d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "#手动定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #加载预训练模型,自动添加分类头,可以直接使用该模型训练\n",
    "        #在线加载的方式,国内的网络得看运气\n",
    "        #更多模型:https://huggingface.co/models\n",
    "        #pretrained = AutoModelForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n",
    "\n",
    "        #保存到磁盘\n",
    "        #pretrained.save_pretrained('model/9.pretrained.model')\n",
    "\n",
    "        #离线加载\n",
    "        pretrained = AutoModelForSequenceClassification.from_pretrained(\n",
    "            './model/9.pretrained.model', num_labels=2)\n",
    "\n",
    "        #锁定参数,不训练\n",
    "        for param in pretrained.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        #只需要特征抽取部分\n",
    "        self.pretrained = pretrained.bert\n",
    "        self.pretrained.eval()\n",
    "\n",
    "        #线性输出层,这部分是要重新训练的\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #调用预训练模型抽取参数,因为预训练模型是不训练的,所以这里不需要计算梯度\n",
    "        with torch.no_grad():\n",
    "            #[8, 200, 768]\n",
    "            out = self.pretrained(\n",
    "                input_ids=data['input_ids'],\n",
    "                token_type_ids=data['token_type_ids'],\n",
    "                attention_mask=data['attention_mask']).last_hidden_state\n",
    "\n",
    "        #只要取第0个字符的特征计算分类结果即可,这和BERT模型本身的训练方式有关系\n",
    "        #[8, 200, 768] -> [8, 768]\n",
    "        out = out[:, 0]\n",
    "\n",
    "        #计算线性输出\n",
    "        #[8, 768] -> [8, 2]\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "#试算\n",
    "model({\n",
    "    'input_ids': torch.ones(8, 200).long(),\n",
    "    'token_type_ids': torch.ones(8, 200).long(),\n",
    "    'attention_mask': torch.ones(8, 200).long(),\n",
    "    'label': torch.ones(8).long(),\n",
    "}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1be5ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cpu\n",
      "0 0.617880642414093 0.625\n",
      "200 0.7409070134162903 0.5\n",
      "400 0.12331488728523254 0.875\n",
      "600 0.1410677433013916 1.0\n",
      "800 0.162505105137825 0.875\n",
      "1000 0.10886094719171524 1.0\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "def train():\n",
    "    #注意这里的参数列表,只包括要训练的参数即可\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "    loss_fun = torch.nn.CrossEntropyLoss()\n",
    "    model.fc.train()\n",
    "\n",
    "    #定义计算设备,优先使用gpu\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    print('device=', device)\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        #如果使用gpu,数据要搬运到显存里\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "\n",
    "        out = model(data)\n",
    "        loss = loss_fun(out, data['label'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            acc = (out.argmax(dim=1) == data['label']).sum().item() / len(\n",
    "                data['label'])\n",
    "            print(i, loss.item(), acc)\n",
    "\n",
    "    #保存模型,只保存训练的部分即可\n",
    "    torch.save(model.fc.to('cpu'), 'model/9.model')\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f73f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90375\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "\n",
    "    #加载保存的模型\n",
    "    model.fc = torch.load('model/9.model')\n",
    "    model.fc.eval()\n",
    "\n",
    "    #加载测试数据集,共10000条数据\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=dataset['test'],\n",
    "                                              batch_size=8,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(100):\n",
    "        data = next(iter(loader_test))\n",
    "\n",
    "        #这里因为数据量不大,使用cpu计算就可以了\n",
    "        out = model(data).argmax(dim=1)\n",
    "\n",
    "        correct += (out == data['label']).sum().item()\n",
    "        total += len(data['label'])\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt2]",
   "language": "python",
   "name": "conda-env-pt2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
